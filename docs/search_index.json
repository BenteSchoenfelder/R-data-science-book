[
["index.html", "R Data Science Book 1 Introduction 1.1 Goals 1.2 Data 1.3 Git and GitHub", " R Data Science Book Tools and Programming Languages for Data Scientists, FH Kiel, Summer Term 2020 2020-04-29 1 Introduction 1.1 Goals This book is a joint effort of the course “Tools and Programming Languages for Data Science”, FH Kiel. We develop this book, in order to learn how typical data wrangling tasks can be solved using the programming language R and in order to practice collaborative programming workflows using Git and GitHub. All of the R packages that we are going to cover are already extensively documented in books, online, and R help files. Our mission for this book is to investigate what these packages are good for, think about good example use cases in the context of data that we know, and apply their functionalities to these data. We start with core tidyverse packages that facilitate the data science workflow: tidying the data using the tidyr package, and exploring the data using the dplyr package. We work on tidyverse packages dedicated for specific data types: stringr for text data, lubridate for dates and times, and forcats for categorical variables. If data sets are huge we may run into performance problems. Hence, we explore the advantages of the data.table package for high performance computing in R. 1.2 Data The whole book is supposed to be based on the data contained in the data subdirectory. Please ask me if you would like to add another data set (e.g. because it would allow you to better demonstrate the functionalities of your package). Currently, the following data sets are covered: diamonds: data on diamonds; 50000 rows; categorical and numeric variables spotify-charts-germany: German daily top 200 charts for one year; 70000 rows; mostly numeric variables and dates olympic-games: data on olympic games medallists; 250000 rows, categorical and numeric data recipes: data on recipes; 60000 rows; character, date, and numeric variables related information weather-kiel-holtenau: weather data for Kiel-Holtenau in 10-Minute intervals for one year, 50000 rows; Date, time and numeric variables. 1.3 Git and GitHub We will work in teams of 2 students per topic. An important part of the book project is practicing collaborative workflows using Git and Github. We will use the Forking Workflow which is typical of open source projects. This involves the following steps: Fork the ‘official’ GitHub repository (“upstream”). This creates your own GitHub copy (“origin”). Clone your fork to your local system. Connect your local clone to the upstream repo by adding a remote path. Create a new local feature branch. Make changes on the new branch and create commits for these changes. Push the branch to your GitHub repo (“origin”) Open a pull request on GitHub to the upstream repository. Your team mate reviews your pull request. Once approved, it is merged into the upstream repo. How to connect your local clone to the upstream repo? # Check the currently registered remote repositories git remote -v # Add the upstream repo git remote add upstream https://github.com/tillschwoerer/R-data-science-book.git How can I integrate changes in the upstream repo into my local system? Best practice is to regularly pull upstream changes into the master branch of your local system, and then create a new feature branch, in which you make your own edits and commits. Never edit the master yourself. If you follow this routine, the pull won’t cause any conflicts - it will be a so called fast forward merge. To be on the save side use git pull upstream master --ff-only. The --ff-only flag means that the upstream changes are merged into the master only if it is a fast forward merge. If you have accidently made commits to the master, you will get an error message. In this case follow the steps described here to resolve the conflict. Further literature: https://happygitwithr.com/fork-and-clone.html https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow "],
["tidyr.html", "2 Tidying data: tidyr", " 2 Tidying data: tidyr "],
["dplyr.html", "3 Wrangling data: dplyr", " 3 Wrangling data: dplyr library(tidyverse) df &lt;- read_csv(&quot;data/diamonds.csv&quot;) ## Parsed with column specification: ## cols( ## carat = col_double(), ## cut = col_character(), ## color = col_character(), ## clarity = col_character(), ## depth = col_double(), ## table = col_double(), ## price = col_double(), ## x = col_double(), ## y = col_double(), ## z = col_double() ## ) df %&gt;% names() ## [1] &quot;carat&quot; &quot;cut&quot; &quot;color&quot; &quot;clarity&quot; &quot;depth&quot; &quot;table&quot; &quot;price&quot; ## [8] &quot;x&quot; &quot;y&quot; &quot;z&quot; df %&gt;% group_by(color) %&gt;% summarise(MEANPRICE = mean(price)) "],
["lubridate.html", "4 Dates and times: lubridate 4.1 Background 4.2 Basics 4.3 Application - Import, Clean Data / date-time 4.4 Application - Create new date-time Variables 4.5 Application - Exploration - Analysis 4.6 Application - Visualisation of core findings 4.7 Wrap up - outlook date-time / time-series 4.8 Wrap up - What’s next/out there?", " 4 Dates and times: lubridate Resources: Lubridate homepage Cheatsheet Book Chapter in R4DS Vignette Suggested data set: weather-kiel-holtenau 4.1 Background 4.1.1 What is Lubridate? Lubridate is an R-Package designed to ease working with date/time variables. These can be quite challenging in baseR and lubridate makes working with dates and times more frictionless, hence the name. Lubridate ist part of the tidyverse package, but can be installed seperately as well. It probably reveals most of its usefullness in collaboration with other tidyverse packages. A useful extension depending on your data might be the time-series package, which is not part of tidyverse. All mentioned packages can be optained with the following commands. package.install(“lubridate”) package.install(“tidyverse”) package.install(“time-series”) If the have been installed previously in your environment, they might have to be called upon by using library(tidyverse) and so forth. 4.2 Basics Some examples of real world date - time formats found in datasets: How people talk about dates and times often differs from the notation of the given information. Depending on the specific use of the data, the given information might be more or less granular. When people in the USA talk distance between two places, they often give an approximation of how long it will take a person to drive from A to B and round-up or down to the hour. Flight schedules will most likely be exact to the minute, while some sensordata will probably need to be exact to the milisecond. So there will be differing granularity in date and time data. Even if this would not be a challenge, we still would have to deal with different notations of date and time. People in Germany will write a day-date like: dd.mm.yyyy or short dd.mm.yy, while the anglo-saxon realm will use mm.dd.yyyy frequently and the most chronologically sound way would be to use yyyy.mm.dd, but this doesn’t seem to stick with humans. On top of these issues there’s the fact that time itself does not make the impression of being the most exact parameter out there. Universal time might appear linear, but the way our planet revolves our galaxy has made it neccessary to adjust date and times every now and then, so our Kalender stays in tune with our defined seasons. This creates leap years, skipped seconds, daylight-savings time and last, but not least time-zones, which can mess things up even further. Three types of date/time data Sys.time() functions Time Formats 4.3 Application - Import, Clean Data / date-time We will apply the lubridate package to Weather data from on stationary sensor in northern Germany, the weather station in Kiel-Holtenau to be more exact. Before we introduce the library, the prerequisites must be created. # zum Lesen von Dateien library(readr) library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date A first step is to import the data, which is given in a csv-format. We will use the tidyverse version of read_csv to accomplish this step. The data will be called df in order to make reference in code an writing more efficient further down. This is the simple approach to read a file with suffix csv. df &lt;- read_csv(&quot;data/weather_kiel_holtenau.csv&quot;) ## Parsed with column specification: ## cols( ## STATIONS_ID = col_double(), ## MESS_DATUM = col_double(), ## TEMPERATUR = col_double(), ## RELATIVE_FEUCHTE = col_double(), ## NIEDERSCHLAGSDAUER = col_double(), ## NIEDERSCHLAGSHOEHE = col_double(), ## NIEDERSCHLAGSINDIKATOR = col_double() ## ) head(df) All columns are recognized as double values. We will examine the data further in a few seconds. At this point it should be mentioned that we generated the code to import the data by using the data import readr tool of RStudio. This tool allows a first look at the raw csv data before import and some tweaking. By looking at the “MESS_DATUM” column it became apparent that this is a timestamp that was created every 10 minutes. The “normal” column type definitions of the import tool would not have sufficed to format this appropriately, which is why we chose to keep the column defined as a double until after import. df$MESS_DATUM &lt;- ymd_hm(df$MESS_DATUM) head(df) When you importing the column “MESS_DATUM” as character, you can use the following function: df_a2 &lt;- read_csv(&quot;data/weather_kiel_holtenau.csv&quot;, col_types = cols(MESS_DATUM = col_character())) df_a2$MESS_DATUM &lt;- parse_date_time(df_a2$MESS_DATUM, orders =&quot;Ymd HM&quot;) head(df_a2) If you no longer want to process or analyze the data, you can also display a date-time object in a specific format. df$MESS_DATUM &lt;- format(df$MESS_DATUM, &quot;%d.%m.%Y-%H:%M&quot;) #TODO # statt format( df$MESS_DATUM, &quot;%Y%m%d%H%M&quot;) geht auch # strftime(Sys.time(), &quot;%Y%m%d%H%M&quot;) # return character # der nächste Ansatz müsste wieder von einer list zu einem character-array gewandelt werden, sonst Anzeigen von NA # strptime(df$MESS_DATUM, &quot;%Y%m%d%H%M&quot;) # strptime(Sys.time(), &quot;%Y%m%d%H%M&quot;) # return list head(df) This approach is ok, but not yet ideal for further processing. The second approach is a little better. You can already specify data types for each column when reading in. For this Step you can use the RStudio “Import Dataset”-Wizard. Is DF immutable? df_a3 &lt;- read_csv(&quot;data/weather_kiel_holtenau.csv&quot;, col_types = cols(MESS_DATUM = col_datetime(format = &quot;%Y%m%d%H%M&quot;), NIEDERSCHLAGSDAUER = col_integer(), NIEDERSCHLAGSINDIKATOR = col_integer(), STATIONS_ID = col_integer())) #View(weather_kiel_holtenau) head(df_a3) (Idea - check, if data is openly available as well. This could lead to an excurse about how to generate a raw dataset from this weatherstation.) Examine the data - - summarise, glimpse an other exploration functions Parse the date-time Variable into a productive format The result should be a tidy data frame. Create an overview over the units of measured variables. possible: Formats and representation of the data. European vs. US number formats etc. possible: check time-series package, if this would help us in any way to make data even more accessible? 4.4 Application - Create new date-time Variables week of year (mutate) separate daytime vs. nighttime separate seasons (spring, summer, fall, winter) 4.5 Application - Exploration - Analysis Calculate Average Temperatures (d/m/season/y) Calculate Averages for humdity, rainfall etc. Plus many more insights, that are not apparent at this draft stage. 4.6 Application - Visualisation of core findings How has the climate changed during the observed intervall? How can relevant intervalls be compared? can we find historic KPIs to compare our findings (eg. average temperature in January in Kiel 1900) What Visualisations make sense for our kind of data/insights? Research and try&amp;error 4.7 Wrap up - outlook date-time / time-series what potential problems have not been adressed? 4.8 Wrap up - What’s next/out there? Insights from data / data vis What potential problems have not been adressed? (Time series package? etc?) "],
["forcats.html", "5 Categorical data: forcats 5.1 Introduction 5.2 General functions 5.3 Combine factors 5.4 Order of levels 5.5 Value of levels 5.6 Add or drop levels", " 5 Categorical data: forcats 5.1 Introduction This chapter is dedicated to the handling of categorical variables. This becomes important if information is to be presented in a non-alphabetical order or aggregated in a meaningful way. Within the R programming language, categorical variables are converted into a form that can be used for analysis using factors. While many Base-R functions automatically convert character vectors into factors, tidyverse requires an explicit treatment. The core-tidyverse provides the package forcats, which will be described here. Further information and exercises are available at the sources shown. Resources: Homepage Cheatsheet Chapter in R4DS Vignette In this chapter we use the demo data set “diamonds” from the ggplot2-package (more information) as well as a dataset “olympic-games” which shows the medal success of olympic athletes from 1896 to 2016. For the latter we focus on the summer games 1896 and the winter games 1924 for practical reasons. Before you start reading, you should have read the chapter Wrangling Data: dplyr or be familiar with this field. library(tidyverse) # the package tidyverse contains all libraries necessary for this chapter diamonds &lt;- read_csv(&quot;data/diamonds.csv&quot;) # Importing the &quot;diamonds&quot; dataset olympic &lt;- read_csv(&quot;data/olympic-games.csv&quot;) # Importing the &quot;olympic-games&quot; dataset head(diamonds, 2) # Have a short look at the data head(olympic, 2) # Have a short look at the data 5.2 General functions 5.2.1 Create Basically two things are needed to create a factor: a vector which contains the values to be analyzed another vector which divides the values into levels As an example we will use the column clarity of the diamonds dataset. It is a categorical evaluation of the clarity of a diamond and a subset of the grades according to the Gemological Institute of America (GIA) grading system. The grades according to GIA read as follows: GIA grade Category Included in diamonds FL Flawless ✘ IF Internally Flawless ✔ VVS1 Very Very Slightly Included1 ✔ VVS2 Very Very Slightly Included2 ✔ VS1 Very Slightly Included1 ✔ VS2 Very Slightly Included2 ✔ SI1 Slightly Included1 ✔ SI2 Slightly Included2 ✔ I1 Included1 ✔ I2 Included2 ✘ I3 Included3 ✘ If you are interested in the distribution of the diamonds in this category, you could do this by using a suitable query: diamonds %&gt;% group_by(clarity) %&gt;% count() %&gt;% # count the number of observations per clarity grade ggplot(aes(clarity, n)) + geom_col() A sorting of the x-axis, which follows the order of the grades as shown in the table above, is unfortunately not possible in this form. A workaround is to convert the column clarity into a factor, which allows us to evaluate the individual categories. For this purpose we first define a vector, which ranks the categories according to their grade (from bad to good): levels_clarity &lt;- c(&quot;I1&quot;, &quot;SI2&quot;, &quot;SI1&quot;, &quot;VS2&quot;, &quot;VS1&quot;, &quot;VVS2&quot;, &quot;VVS1&quot;, &quot;IF&quot;) In the next step we lay this newly created levels vector on the column clarity of our data set to create a factor. factor() is a base-R function. factor_clarity &lt;- factor(x = diamonds$clarity, levels = levels_clarity) factor_clarity consists solely of the clarity column of the original diamonds data set. Another possibility to create a factor is to convert a vector using as_factor(). Here the levels are created automatically. Their order however depends on the appearance of the corresponding value in the source vector. as_factor(diamonds$clarity) %&gt;% levels() ## [1] &quot;SI2&quot; &quot;SI1&quot; &quot;VS1&quot; &quot;VS2&quot; &quot;VVS2&quot; &quot;VVS1&quot; &quot;I1&quot; &quot;IF&quot; 5.2.2 Count values per level Now we repeat the analysis of the distribution within our dataset using the created factor. The function fct_count() returns the frequency of a categorical value within a factor. The order of the levels remains unchanged. fct_count(factor_clarity) %&gt;% ggplot(aes(clarity, n)) + #the definition of aes. mapping in this line is only used to label the axes geom_col(aes(f, n)) It becomes obvious that the distribution can now be displayed in the desired order (by valence). Functions of the package forcats always start with the prefix fct_. 5.2.3 Inspect and set levels With the function levels() the levels of a factor can be both read and defined. But be aware: this can only be used to change the names, not the order of the levels. The base-R function unclass() gives information about the internal memory structure of a factor. factor_clarity %&gt;% levels() #shows the original levels of our factor ## [1] &quot;I1&quot; &quot;SI2&quot; &quot;SI1&quot; &quot;VS2&quot; &quot;VS1&quot; &quot;VVS2&quot; &quot;VVS1&quot; &quot;IF&quot; factor_clarity[1:25] %&gt;% unclass() #shows the internal structure ## [1] 2 3 5 4 2 6 7 3 4 5 3 5 3 2 2 1 2 3 3 3 2 4 5 3 3 ## attr(,&quot;levels&quot;) ## [1] &quot;I1&quot; &quot;SI2&quot; &quot;SI1&quot; &quot;VS2&quot; &quot;VS1&quot; &quot;VVS2&quot; &quot;VVS1&quot; &quot;IF&quot; #remain the semantic order but replace the technical jargon with something understandable levels_clarity_c &lt;- c(&quot;c8&quot;, &quot;c7&quot;, &quot;c6&quot;, &quot;c5&quot;, &quot;c4&quot;, &quot;c3&quot;, &quot;c2&quot;, &quot;c1&quot;) levels(factor_clarity) &lt;- levels_clarity_c factor_clarity %&gt;% head(25) ## [1] c7 c6 c4 c5 c7 c3 c2 c6 c5 c4 c6 c4 c6 c7 c7 c8 c7 c6 c6 c6 c7 c5 c4 c6 c6 ## Levels: c8 c7 c6 c5 c4 c3 c2 c1 5.2.4 Inspect unique values The function fct_unique() can be used to output unique values of a factor. In contrast to the base-R function unique() the values are returned in the order of the levels and not in the order of their appearance. factor_clarity %&gt;% fct_unique() ## [1] c8 c7 c6 c5 c4 c3 c2 c1 ## Levels: c8 c7 c6 c5 c4 c3 c2 c1 factor_clarity %&gt;% unique() ## [1] c7 c6 c4 c5 c3 c2 c8 c1 ## Levels: c8 c7 c6 c5 c4 c3 c2 c1 5.3 Combine factors 5.3.1 Combine factors with different levels With the function fct_c() factors with different levels can be combined to one factor covering all levels. First we create two factors. The first one shows the amount of contested disciplines by several countries during the olympic sommer games 1896. The second one shows the amount of contested disciplines by countries during the olympic winter games 1924. Note, that the default option for the levels argument in factor() is a sorted set of the given values. As R sorts characters lexicographically. the corresponding levels are the countries sorted alphabetically from A to Z. olympic_1896 &lt;- olympic %&gt;% filter(game == &quot;1896 Summer&quot;) # Using default option for `levels` argument in factor() countries_in_1896 &lt;- factor(olympic_1896$country) olympic_1924 &lt;- olympic %&gt;% filter(game == &quot;1924 Winter&quot;) # The default option for the `levels` argument in factor countries_in_1924 &lt;- factor(olympic_1924$country) The factors differ both in their stored categorical data and in their set levels. A plot makes this clear: countries_in_1896 %&gt;% fct_count() %&gt;% ggplot(aes(`amount of disciplines`, country))+ geom_col(aes(n, f)) countries_in_1924 %&gt;% fct_count() %&gt;% ggplot(aes(`amount of disciplines`, country))+ geom_col(aes(n, f)) Now we combine the factors with fct_c() and plot it: fct_c(countries_in_1896, countries_in_1924) %&gt;% fct_count() %&gt;% ggplot(aes(`amount of disciplines`, country))+ geom_col(aes(n,f)) Both the underlying categorical data and the defined levels were combined in one factor. 5.3.2 Standardise levels of various factors With the function fct_unify() the levels of different factors can be standardised. Please note, both input and output are in list form. factor_list &lt;- fct_unify(list(countries_in_1896, countries_in_1924)) # plotting the first factor factor_list[[1]] %&gt;% fct_count() %&gt;% ggplot() + geom_col(aes(n, f)) # plotting the second factor factor_list[[2]] %&gt;% fct_count() %&gt;% ggplot() + geom_col(aes(n, f)) In this case, the underlying categorical data was left unchanged for both factors, but the levels were standardised. This is especially useful when comparing the categorical data of two different factors. 5.4 Order of levels 5.4.1 Manual reordering of levels With the function fct_relevel() the levels of a factor can be reordered. In contrast to the function levels(), which only allows the renaming of factor levels, fct_relevel() also adjusts the order of the levels themselves, that is the way they are stored internally. An example should clarify this. # have a look at the original levels factor_clarity %&gt;% fct_count() # The wrong approach (this is semantical wrong): levels(factor_clarity) &lt;- c(&quot;c1&quot;,&quot;c3&quot;,&quot;c5&quot;,&quot;c7&quot;,&quot;c2&quot;,&quot;c4&quot;,&quot;c6&quot;,&quot;c8&quot;) factor_clarity %&gt;% fct_count() #tidy up levels(factor_clarity) &lt;- levels_clarity_c # The right approach: factor_clarity %&gt;% fct_relevel(c(&quot;c1&quot;,&quot;c3&quot;,&quot;c5&quot;,&quot;c7&quot;,&quot;c2&quot;,&quot;c4&quot;,&quot;c6&quot;,&quot;c8&quot;)) %&gt;% fct_count() As you can see, only the function fct_relevel() allows a correct relevel. 5.4.2 Reordering by frequency Especially for plots it is often useful to orient the order of the levels on the frequency of the corresponding values. The function fct_infreq() allows exactly this. Plotting the unsorted factor leads to a difficult readability. countries_in_1896 %&gt;% fct_count() %&gt;% ggplot(aes(n, f)) + geom_col() The better approach is to sort the data before plotting. countries_in_1896 %&gt;% fct_infreq() %&gt;% #insert fct_infreq() to get the data ordered fct_count() %&gt;% ggplot(aes(n, f)) + geom_col() 5.4.3 Reordering by appearance The package forcats offers with the function fct_inorder() the possibility to orientate the order of the levels of a factor to the occurence of the corresponding categorical data in the data set. To make this clear, we take a look at the unique values with the first 20 entries in the corresponding data set: olympic_1896$country %&gt;% head(20) %&gt;% unique() ## [1] &quot;Greece&quot; &quot;Great Britain&quot; &quot;Switzerland&quot; &quot;United States&quot; ## [5] &quot;Germany&quot; Create a factor in the appropriate order of levels: olympic_1896$country %&gt;% fct_inorder() %&gt;% levels() ## [1] &quot;Greece&quot; &quot;Great Britain&quot; &quot;Switzerland&quot; &quot;United States&quot; ## [5] &quot;Germany&quot; &quot;France&quot; &quot;Hungary&quot; &quot;Australia&quot; ## [9] &quot;Austria&quot; &quot;Denmark&quot; &quot;Italy&quot; &quot;Sweden&quot; 5.4.4 Reverse level order The function fct_rev() reverses the existing order of the levels of a factor. First have a look at the original order: countries_in_1896 %&gt;% fct_infreq() %&gt;% #insert fct_infreq() to get the data ordered fct_count() %&gt;% ggplot(aes(n, f)) + geom_col() Now we reverse the order: countries_in_1896 %&gt;% fct_infreq() %&gt;% #insert fct_infreq() to get the data ordered fct_rev() %&gt;% #insert fct_rev() to reverse this order fct_count() %&gt;% ggplot(aes(n, f)) + geom_col() 5.4.5 Shift levels The argument n in the function fct_shift() allows to shift the levels to the left (right) for negative (positive) integer values of n, wrapping around end. Thus a value of n = -1L would shift the order of the levels to the left by one location. countries_in_1896 %&gt;% fct_infreq() %&gt;% #insert fct_infreq() to get the data ordered fct_shift(n = -1L) %&gt;% #insert fct_shift() to shift levels fct_count() %&gt;% ggplot(aes(n, f)) + geom_col() 5.4.6 Randomly permute levels The level of a factor can also be randomly shuffeled using fct_shuffle(). The input argument can be either a factor or a character vector, whereas the output will be a factor. By way of example, this is demonstrated using the factor countries_in_1896: orig_levels &lt;- countries_in_1896 %&gt;% levels() # the original levels are sorted alphabetically orig_levels %&gt;% print() ## [1] &quot;Australia&quot; &quot;Austria&quot; &quot;Denmark&quot; &quot;France&quot; ## [5] &quot;Germany&quot; &quot;Great Britain&quot; &quot;Greece&quot; &quot;Hungary&quot; ## [9] &quot;Italy&quot; &quot;Sweden&quot; &quot;Switzerland&quot; &quot;United States&quot; shuffled_factor &lt;- orig_levels %&gt;% fct_shuffle() shuffled_levels &lt;- shuffled_factor %&gt;% levels() # the shuffled levels are randomly sorted shuffled_levels %&gt;% print() ## [1] &quot;Denmark&quot; &quot;France&quot; &quot;Switzerland&quot; &quot;United States&quot; ## [5] &quot;Australia&quot; &quot;Hungary&quot; &quot;Sweden&quot; &quot;Greece&quot; ## [9] &quot;Germany&quot; &quot;Italy&quot; &quot;Austria&quot; &quot;Great Britain&quot; 5.4.7 Reordering levels by other variables The functions presented in this section bare great similarity to the fct_relevel() function introduced in the beginning of this section. fct_relevel() allows for a direct manipuation of the levels by passing the new order to the levels keyword argument. In this regard fct_reorder() and fct_reorder2() are different, as the levels are reordered according to the result of a function applied to a vector. The example below is intended to unravel this behavior. # TODO: Need to think of an example here. # fct_reorder() for fun(x); fct_reorder2() for fun(x, y) 5.5 Value of levels 5.6 Add or drop levels "],
["stringr.html", "6 Character data: stringr", " 6 Character data: stringr Resources: Homepage Book Chapter in R4DS Cheatsheet Vignette Into Vignette Regular Expressions Suggested data: recipes Advice: Many of the examples in the Vignettes just refer to vectors. How can we use stringr to create/modify character columns of a data frame? "],
["data-table.html", "7 High performance computing: data.table 7.1 Subsetting rows", " 7 High performance computing: data.table Resources: Vignette Intro Vignette on reference semantics Github Wiki Cheatsheet data.table vs. dplyr Suggested data: any Possible approach: take different sequences of operations in dplyr and translate it into data.table syntax. Then summarise differences and pros/cons vis-à-vis dplyr Okay, let’s get started with this chapter on high performance computing using the R package data.table. First we will install exactly this package. print(&quot;Hello Data.Table&quot;) ## [1] &quot;Hello Data.Table&quot; install.packages(&quot;data.table&quot; , repos=&quot;http://cran.us.r-project.org&quot;) ## Installing package into &#39;C:/Users/pizza/OneDrive/Dokumente/R/win-library/3.6&#39; ## (as &#39;lib&#39; is unspecified) ## package &#39;data.table&#39; successfully unpacked and MD5 sums checked ## ## The downloaded binary packages are in ## C:\\Users\\pizza\\AppData\\Local\\Temp\\RtmpGOX5WB\\downloaded_packages library(tidyverse) library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose inpOlympic &lt;- if (file.exists(&quot;data/olympic-games.csv&quot;)) { &quot;data/olympic-games.csv&quot; } inpWeather &lt;- if (file.exists(&quot;data/weather_kiel_holtenau.csv&quot;)) { &quot;data/weather_kiel_holtenau.csv&quot; } inpSpoti &lt;- if (file.exists(&quot;data/spotify_charts_germany.csv&quot;)) { &quot;data/spotify_charts_germany.csv&quot; } olympics &lt;- fread(inpOlympic) weather &lt;- fread(inpWeather) ## Warning in require_bit64_if_needed(ans): Some columns are type &#39;integer64&#39; ## but package bit64 is not installed. Those columns will print as strange ## looking floating point data. There is no need to reload the data. Simply ## install.packages(&#39;bit64&#39;) to obtain the integer64 print method and print the ## data again. spotify &lt;- fread(inpSpoti) 7.0.1 Data exploration Datatype data.table class(olympics) ## [1] &quot;data.table&quot; &quot;data.frame&quot; 7.1 Subsetting rows We can condition on the rows one time swimmers &lt;- olympics[sport == &quot;Swimming&quot;] and two or more times swimmers &lt;- olympics[sport == &quot;Swimming&quot; &amp; medal == &quot;Gold&quot;] ordering data like var &lt;- swimmers[order(height)] Now we want to condition on the colums too cols &lt;- c(&quot;athlete&quot;, &quot;medal&quot;) or let us use regualr expression to find someone ledecky &lt;- swimmers[athlete %like% &quot;Ledecky&quot;] "]
]
